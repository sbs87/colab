# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1syeP3pxb1pzmdIGj_0H6dZtHOU7aBXNb

# Linear regression
## Install TF 2.0
"""

!pip install -q tensorflow==2.0.0-beta

"""## Import packages"""

import tensorflow as tf
print(tf.__version__)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""## Get & Load data"""

!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv

!ls
!head moore.csv

data = pd.read_csv("moore.csv",header=None).values
data

"""## Format & transform data

*   convert to log scale
*   x-center


"""

x_data=data[:,0].reshape(-1,1)
y_data=data[:,1]
plt.scatter(x_data,y_data)

y_data_log=np.log(y_data)
plt.scatter(x_data,y_data_log)

x=x_data-x_data.mean()
y=y_data_log
plt.scatter(x,y)

"""## Build and compile model"""

model=tf.keras.Sequential([tf.keras.layers.Input(shape=(1,)),
                     tf.keras.layers.Dense(1)])

model.compile(optimizer=tf.keras.optimizers.SGD(0.001,0.9),loss='mse')

## What happens at different learning rates?

def schedule(epoch,lr):
  if epoch<50:
    return 0.0001
  return 0.001

schedule(100,2)

scheduler=tf.keras.callbacks.LearningRateScheduler(schedule)
scheduler

"""## Fit model, evaluate"""

r=model.fit(x,y,epochs=200,callbacks=[scheduler])

plt.plot(r.history['loss'],label='loss vs epoch')

type(r)

## How would I know which layer corresponse to dense? 
## It seems like layers[0] and ommitting it yeilds same results

#model.get_weights()
print(model.layers)
print(model.layers[0].get_weights())
slope=model.layers[0].get_weights()[0][0,0] # wow this is dangerous. IS there a better way to do this in python? Should be labeled. 
print(slope)
b=model.get_weights()[1][0]
print(b)

# time to double
np.log(2)/slope

"""## "Predict" i.e., plot fit line with actual values"""

predicted=x*slope+b
plt.scatter(x,y)
plt.plot(x,predicted,'r--')